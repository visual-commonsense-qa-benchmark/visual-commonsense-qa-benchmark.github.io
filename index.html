<!DOCTYPE html>

<html>
<head>
<meta charset="utf-8"/>
<meta content="VCQA: A Visual Commonsense Benchmark" name="description"/>
<meta content="VCQA Benchmark" property="og:title"/>
<meta content="VCQA: A Visual-Feedback Question Answering Benchmark for Commonsense Reasoning" property="og:description"/>
<meta content="https://visual-commonsense-qa-benchmark.github.io/" property="og:url"/>
<meta content="static/image/your_banner_image.png" property="og:image"/>
<meta content="1200" property="og:image:width"/>
<meta content="640" property="og:image:height"/>
<meta content="VCQA Benchmark" name="twitter:title">
<meta content="Visual Commonsense Question Answering Benchmark for Vision-Language Models" name="twitter:description">
<meta content="static/images/nitzangu_Albert_Einstein_holds_an_iphone_realistic_style_ba15ea7b-b829-478b-adef-b474ccf01a2a.png" name="twitter:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Vision-and-Language Benchmark, Synthetic and Compositional Images" name="keywords"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>VCQA: Visual Commonsense Question Answering Benchmark</title>
<link href="static/images/icon1.png" rel="icon" type="image/x-icon"/>
<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"/>
<link href="static/css/bulma.min.css" rel="stylesheet"/>
<link href="static/css/bulma-carousel.min.css" rel="stylesheet"/>
<link href="static/css/bulma-slider.min.css" rel="stylesheet"/>
<link href="static/css/fontawesome.all.min.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet"/>
<link href="static/css/index.css" rel="stylesheet"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
<script defer="" src="static/js/fontawesome.all.min.js"></script>
<script src="static/js/bulma-carousel.min.js"></script>
<script src="static/js/bulma-slider.min.js"></script>
<script src="static/js/index.js"></script>
</meta></meta></head>
<body>
<section class="hero">
<div class="hero-body">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column has-text-centered">
<h1 class="title is-1 publication-title">VCQA: A Visual Feedback Benchmark for Commonsense Reasoning</h1>
<div class="is-size-5 publication-authors"><span class="author-block">Nitzan Guetta, Noam Tarshish, Nofar Selouk, Eliya Aharon</span><br/><span class="author-block">Ben-Gurion University of the Negev</span></div>
<div class="is-size-5 publication-authors">
<br/>
<span class="author-block">
<b>
</b>
</span>
<br/>
</div>
<br/>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2407.19474" target="_blank">
<span class="icon">
<i class="ai ai-arxiv"></i>
</span>
<span>arXiv</span>
</a>
</span>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://huggingface.co/datasets/visual-riddles/visual_riddles" target="_blank">
<span class="icon">
<p style="font-size:20px">ðŸ¤—</p>
</span>
<span>Dataset</span>
</a>
</span>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://huggingface.co/spaces/visual-riddles/visual-riddles" target="_blank">
<span class="icon">
<p style="font-size:20px">ðŸ¤—</p>
</span>
<span>Explorer</span>
</a>
</span>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://colab.research.google.com/drive/1E3Kpc9xFkD97CEobN79C9jdLJNIcbz0s?usp=sharing" target="_blank">
<span class="icon">
<i class="fa fa-code"></i>
</span>
<span>Evaluation Notebook</span>
</a>
</span>
</div>
</div>
</div>
</div>
</section>
<section class="section hero is-light">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="publication-flipped">
<div class="content has-text-justified">
<div class="flip-box-container">
<div class="flip-box">
<center><h4><br/>Where would I place my paper plate?<br/><br/></h4></center>
<div class="flip-box-inner">
<div class="flip-box-front">
<img alt="Paris" src="static/images/paper.png" style="width:300px;height:300px"/>
</div>
<div class="flip-box-back">
<img alt="Paper plate with bounding box" src="static/images/paperbb.png" style="width:300px;height:300px"/>
</div>
</div>
</div>
<div class="flip-box">
<center><h4><br/>What would you use to locate a highway or street?<br/><br/></h4></center>
<div class="flip-box-inner">
<div class="flip-box-front">
<img alt="Paris" src="static/images/phone.png" style="width:300px;height:300px"/>
</div>
<div class="flip-box-back">
<img src="static/images/phonebb.png" style="width:300px;height:300px"/>
</div>
</div>
</div>
<div class="flip-box">
<center><h4><br/>Where would you put your foot if it is already covered in cloth?<br/><br/></h4></center>
<div class="flip-box-inner">
<div class="flip-box-front">
<img alt="Paris" src="static/images/shoes.png" style="width:300px;height:300px"/>
</div>
<div class="flip-box-back">
<img src="static/images/shoesbb.png" style="width:300px;height:300px"/>
</div>
</div>
</div>
</div>
</div></div></div></div></section>
<section class="hero is-small">
<div class="hero-body">
<div class="container">
<div class="carousel results-carousel" id="results-carousel">
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Abstract
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left has-background-info-light pr-4 pl-4 pt-3 pb-3">
<p>VCQA is a benchmark for evaluating vision-and-language models on visual commonsense reasoning. Instead of answering with text, models must identify a bounding box that visually supports the correct answer to a free-form question grounded in a synthetic image. This setup directly tests the model's visual inference capabilities while minimizing reliance on language generation. We evaluate both human and model performance on this task and propose metrics that combine IoU with manual correctness ratings.
<img alt="MY ALT TEXT" src="static/images/abstract.png"/>
</p>
</h3>
</div>
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Data Collection
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>The dataset is built using questions from the CommonsenseQA benchmark. Each question is transformed into a visual scenario using a language model that generates scene descriptions for image synthesis. These prompts are refined by humans to ensure solvability and are categorized into commonsense types. Each example includes a free-form question and a ground-truth bounding box indicating the correct answer region.
<img alt="MY ALT TEXT" src="static/images/dataset.png"/>
</p>
</h3>
</div>
<!-- Prompt + Benchmark Task -->
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
    Prompt + Benchmark Task
  </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>
      Each question in our benchmark is paired with a custom-crafted prompt designed to generate synthetic scenes. The prompts are created using LLMs and then manually refined to ensure solvability and visual clarity. The goal is to visually support only one correct answer out of the candidate options.
      <br/><br/>
      (A) Prompt Generation: The prompt is constructed such that only the correct option is visually supported, and distractors are ruled out.<br/>
      (B) Benchmark Task: Given the image and question, participants are required to draw a bounding box over the correct answer.
      <br/><br/>
<img alt="Prompt and Benchmark Figure" src="static/images/fig_prompt_plus_benchmark.jpg"/>
</p>
</h3>
</div>
<!-- Human Annotations Interface -->
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
    Human Annotations Interface
  </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>
      Human annotators were instructed to select the bounding box that best represents the answer to a question, based on visual evidence. The interface was designed to support both annotation and visual validation.
      <br/><br/>
<img alt="Human Ratings Figure" src="static/images/fig_human_ratings.jpg"/>
</p>
</h3>
</div>
<!-- VCQA Example Instance -->
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
    VCQA Instance Example
  </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>
      Each VCQA instance includes a question, candidate answers, and a synthetic image where only one answer is visually correct. The visual scene composition is intentionally misleading to test commonsense reasoning.
      <br/><br/>
<img alt="VCQA Instance Figure" src="static/images/fig1_vcqa.jpg"/>
</p>
</h3>
</div>
</div>
</div>
</div>
</section>
<script src="https://gradio.s3-us-west-2.amazonaws.com/4.37.2/gradio.js" type="module"></script>
<section class="hero is-small is-light">
<div class="hero-body">
<div class="container">
<div class="columns is-centered has-text-centered">

</div>
</div>
</div>
</section>
<script src="https://gradio.s3-us-west-2.amazonaws.com/3.36.1/gradio.js" type="module"></script>
<section class="hero is-small">
<div class="hero-body">
<div class="container">
<div class="columns is-centered has-text-centered">

</div>
</div>
</div>
</section>
<section class="hero is-small is-light">
<div class="hero-body">

<!-- Benchmark Results -->
<div class="hero-body">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="column is-full">
<h2 class="title is-4">Benchmark Performance</h2>
<figure>
<img alt="Table 1: Model Performance" src="static/images/table1.jpg"/>
<figcaption>Table 1: Performance of models and humans on the Visual Feedback VQA task using Majority Vote IoU score. Human baseline leads significantly.</figcaption>
</figure>
<br/>
<figure>
<img alt="Table 2: Human IoU Method Comparison" src="static/images/table2.jpg"/>
<figcaption>Table 2: Comparison of IoU calculation methods for human annotations. Majority Vote is selected for robustness.</figcaption>
</figure>
</div>
</div>
</div>
</div>
<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">BibTeX</h2>
<pre><code>@misc{2025vcqa,
  title={VCQA: A Visual-Feedback Question Answering Benchmark for Commonsense Reasoning},
  author={Nitzan Guetta and Noam Tarshish and Nofar Selouk and Eliya Aharon},
  year={2025},
  institution={Ben-Gurion University of the Negev},
  note={Preprint}
}</code></pre>
</div>
</section>
</div>
</section>
<footer class="footer">
<div class="container">
<div class="columns is-centered">
<div class="column is-8">
<div class="content">
<p style="color:gray;font-size:9.9px;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>, is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <a href="https://www.flaticon.com/free-icons/magic" title="magic icons">Magic icons created by Freepik - Flaticon</a>
</p>
</div>
</div>
</div>
</div>
</footer>
</body>
</html>