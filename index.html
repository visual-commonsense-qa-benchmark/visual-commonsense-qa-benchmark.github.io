<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/>
<meta content="VCQA: A Visual Commonsense Benchmark" name="description"/>
<meta content="VCQA Benchmark" property="og:title"/>
<meta content="VCQA: A Visual-Feedback Question Answering Benchmark for Commonsense Reasoning" property="og:description"/>
<meta content="https://visual-commonsense-qa-benchmark.github.io/" property="og:url">
<meta content="static/image/your_banner_image.png" property="og:image">
<meta content="1200" property="og:image:width"/>
<meta content="640" property="og:image:height"/>
<meta content="VCQA Benchmark" name="twitter:title"/>
<meta content="Visual Commonsense Question Answering Benchmark for Vision-Language Models" name="twitter:description"/>
<meta content="static/images/nitzangu_Albert_Einstein_holds_an_iphone_realistic_style_ba15ea7b-b829-478b-adef-b474ccf01a2a.png" name="twitter:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="Vision-and-Language Benchmark, Synthetic and Compositional Images" name="keywords"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<title>VCQA: Visual Commonsense Question Answering Benchmark</title>
<link href="static/images/vr_icon.png" rel="icon" type="image/x-icon"/>
<link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"/>
<link href="static/css/bulma.min.css" rel="stylesheet"/>
<link href="static/css/bulma-carousel.min.css" rel="stylesheet"/>
<link href="static/css/bulma-slider.min.css" rel="stylesheet"/>
<link href="static/css/fontawesome.all.min.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet"/>
<link href="static/css/index.css" rel="stylesheet"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
<script defer="" src="static/js/fontawesome.all.min.js"></script>
<script src="static/js/bulma-carousel.min.js"></script>
<script src="static/js/bulma-slider.min.js"></script>
<script src="static/js/index.js"></script>
</head>
<body>
<section class="hero">
<div class="hero-body">
<div class="container is-max-desktop">
<div class="columns is-centered">
<div class="column has-text-centered">
<h1 class="title is-1 publication-title">VCQA: A Visual Feedback Benchmark for Commonsense Reasoning</h1>
<div class="is-size-5 publication-authors"><span class="author-block">Nitzan Guetta, Noam Tarshish, Nofar Selouk, Eliya Aharon</span><br/><span class="author-block">Ben-Gurion University of the Negev</span></div>
<div class="is-size-5 publication-authors">
<br/>
<span class="author-block">
<b>
</b>
</span>
<br/>
</div>
<br/>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://arxiv.org/abs/2407.19474" target="_blank">
<span class="icon">
<i class="ai ai-arxiv"></i>
</span>
<span>arXiv</span>
</a>
</span>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://huggingface.co/datasets/visual-riddles/visual_riddles" target="_blank">
<span class="icon">
<p style="font-size:20px">ðŸ¤—</p>
</span>
<span>Dataset</span>
</a>
</span>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://huggingface.co/spaces/visual-riddles/visual-riddles" target="_blank">
<span class="icon">
<p style="font-size:20px">ðŸ¤—</p>
</span>
<span>Explorer</span>
</a>
</span>
<span class="link-block">
<a class="external-link button is-normal is-rounded is-dark" href="https://colab.research.google.com/drive/1E3Kpc9xFkD97CEobN79C9jdLJNIcbz0s?usp=sharing" target="_blank">
<span class="icon">
<i class="fa fa-code"></i>
</span>
<span>Evaluation Notebook</span>
</a>
</span>
</div>
</div>
</div>
</div>
</section>

<section class="section hero is-light">
<div class="container is-max-desktop">
<div class="columns is-centered has-text-centered">
<div class="publication-flipped">
<div class="content has-text-justified">
<div class="flip-box-container">
<div class="flip-box">
<center><h4><br/>Where would I place my paper plate?<br/><br/></h4></center>
<div class="flip-box-inner">
<div class="flip-box-front">
<img alt="Paris" src="static/images/paper.png" style="width:300px;height:300px"/>
</div>
<div class="flip-box-back">
<img alt="Paper plate with bounding box" src="static/images/paperbb.png" style="width:300px;height:300px"/>
</div>
</div>
</div>
<div class="flip-box">
<center><h4><br/>What would you use to locate a highway or street?<br/><br/></h4></center>
<div class="flip-box-inner">
<div class="flip-box-front">
<img alt="Paris" src="static/images/phone.png" style="width:300px;height:300px"/>
</div>
<div class="flip-box-back">
<img src="static/images/phonebb.png" style="width:300px;height:300px"/>
</div>
</div>
</div>
<div class="flip-box">
<center><h4><br/>Where would you put your foot if it is already covered in cloth?<br/><br/></h4></center>
<div class="flip-box-inner">
<div class="flip-box-front">
<img alt="Paris" src="static/images/shoes.png" style="width:300px;height:300px"/>
</div>
<div class="flip-box-back">
<img src="static/images/shoesbb.png" style="width:300px;height:300px"/>
</div>
</div>
</div>
</div>
</section>

<section class="hero is-small">
<div class="hero-body">
<div class="container">
<div class="carousel results-carousel" id="results-carousel">
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Abstract
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left has-background-info-light pr-4 pl-4 pt-3 pb-3">
<p>VCQA is a benchmark for evaluating vision-and-language models on visual commonsense reasoning. Instead of answering with text, models must identify a bounding box that visually supports the correct answer to a free-form question grounded in a synthetic image. This setup directly tests the model's visual inference capabilities while minimizing reliance on language generation. We evaluate both human and model performance on this task and propose metrics that combine IoU with manual correctness ratings.
<img alt="MY ALT TEXT" src="static/images/abstract.png"/>
</p>
</h3>
</div>
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Data Collection
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>The dataset is built using questions from the CommonsenseQA benchmark. Each question is transformed into a visual scenario using a language model that generates scene descriptions for image synthesis. These prompts are refined by humans to ensure solvability and are categorized into commonsense types. Each example includes a free-form question and a ground-truth bounding box indicating the correct answer region.
<img alt="MY ALT TEXT" src="static/images/dataset.png"/>

</p>
</h3>
</div>
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Visual Riddles Benchmark
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>
         This study introduces three critical tasks within the Visual Riddles benchmark to evaluate vision-and-language models:
           solving open-ended visual riddles, selecting the correct answer from multiple options, and assessing open-ended responses both with and without reference answers.
           We also incorporate auxiliary information, such as textual hints and attributions, to enhance model accuracy.
           For example, hints like 'Look at the colors of the fur' guide models to accurately infer a cat's gender, leveraging knowledge that calico cats are predominantly female.
           These tasks are designed to enhance model capabilities in integrating visual data with commonsense reasoning and detailed justifications, supporting scalable and automated evaluations.
           <br/><br/>
<img alt="MY ALT TEXT" src="static/images/VR_Tasks.jpg"/>
</p>
</h3>
</div>
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Experiments - Open-ended VQA
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>
         Our experiments evaluated several leading vision-and-language models, including LLaVA, Gemini-Pro,
           InstructBLIP, and GPT-4, on various tasks within our benchmark.
          Models like Gemini-Pro-1.5 showed a performance of 40%, with humans achieving 82%. Even with auxiliary data such as human-generated captions, model performance improved only marginally.
           This task tests models' ability to generate correct answers from visual cues alone.
         <br/><br/>
<img alt="MY ALT TEXT" src="static/images/table1.jpg"/>
</p>
</h3>
</div>
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Experiments - Multiple-choice VQA
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>
         This task shifts from generative to classification-based evaluation.
           GPT-4 and Gemini-Pro-Vision showed the highest accuracies, with slight improvements over open-ended tasks.
           Models perform better with hints, demonstrating the importance of auxiliary information in enhancing accuracy.
         <br/><br/>
<img alt="MY ALT TEXT" src="static/images/table2.jpg"/>
</p>
</h3>
</div>
<div class="item">
<h2 class="subtitle is-size-3-tablet has-text-weight-bold has-text-centered has-background-info-light mr-0 pt-3 pb-3">
           Experiments - Automatic Evaluation
           </h2>
<h3 class="subtitle is-size-4-tablet has-text-left pr-4 pl-4 pt-3 pb-3">
<p>
         Gemini-Pro-1.5, identified as the best auto-rater, scored higher in evaluations that used reference-based scenarios.
           It demonstrated that models generally perform better with hints but struggle with attributions, highlighting ongoing challenges in model reasoning with auxiliary data.
         <br/><br/>
<img alt="MY ALT TEXT" src="static/images/table34.jpg"/>
</p>
</h3>
</div>
</div>
</div>
</div>
</section>

<script src="https://gradio.s3-us-west-2.amazonaws.com/4.37.2/gradio.js" type="module"></script>
<section class="hero is-small is-light">
<div class="hero-body">
<div class="container">
<div class="columns is-centered has-text-centered">
<div class="publication-video">
<gradio-app src="https://visual-riddles-visual-riddles.hf.space"></gradio-app>
</div>
</div>
</div>
</div>
</section>

<script src="https://gradio.s3-us-west-2.amazonaws.com/3.36.1/gradio.js" type="module"></script>
<section class="hero is-small">
<div class="hero-body">
<div class="container">
<div class="columns is-centered has-text-centered">
<div class="publication-video">
<gradio-app src="https://visual-riddles-visual-riddles-leaderboard.hf.space"></gradio-app>
</div>
</div>
</div>
</div>
</section>

<section class="hero is-small is-light">
<div class="hero-body">
<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">BibTeX</h2>
<pre><code>@misc{2025vcqa,
  title={VCQA: A Visual-Feedback Question Answering Benchmark for Commonsense Reasoning},
  author={Nitzan Guetta and Noam Tarshish and Nofar Selouk and Eliya Aharon},
  year={2025},
  institution={Ben-Gurion University of the Negev},
  note={Preprint}
}</code></pre>
</div>
</section>
</div>
</section>

<footer class="footer">
<div class="container">
<div class="columns is-centered">
<div class="column is-8">
<div class="content">
<p style="color:gray;font-size:9.9px;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>, is licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/" rel="license" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <a href="https://www.flaticon.com/free-icons/magic" title="magic icons">Magic icons created by Freepik - Flaticon</a>
</p>
</div>
</div>
</div>
</div>
</footer>

</body>
</html>